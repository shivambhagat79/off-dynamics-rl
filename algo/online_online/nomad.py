# =================================================================================================
# NOMAD: Combining DARC and LIBERTY for Unsupervised Domain Adaptation in Reinforcement Learning
#
# This file implements the NOMAD agent, which integrates two key strategies for adapting a policy
# from a source domain to a target domain without access to rewards in the target domain.
#
# The agent is built upon Soft Actor-Critic (SAC) and leverages a dual-policy system:
#
# 1. Exploration Policy:
#    - Interacts with the target environment.
#    - Trained using intrinsic rewards generated by LIBERTY, which encourages discovering
#      novel and diverse states in the target domain.
#
# 2. Main Policy (Source Policy):
#    - This is the final policy intended for evaluation.
#    - Trained on a combination of source and target domain data:
#      a) On source data, it uses a DARC reward correction to penalize transitions that are
#         too "source-like," encouraging it to learn dynamics-agnostic behaviors.
#      b) On target data, it learns from the experiences of the exploration policy, using
#         Importance Sampling (IS) to correct for the policy mismatch.
#
# The overall architecture is designed to be a drop-in replacement for the original agent,
# maintaining compatibility with the existing training framework.
# =================================================================================================

import copy
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, TransformedDistribution, constraints
from torch.distributions.transforms import Transform

# Import the models required for LIBERTY intrinsic reward generation.
from liberty_models import MetricModel, InverseDynamicsModel, ForwardDynamicsModel


# =================================================================================================
# --- Helper Classes and Neural Network Modules ---
# =================================================================================================

class TanhTransform(Transform):
    """
    A transformation that squashes a real-valued input to the range [-1, 1] using the tanh function.
    This is used in SAC to ensure actions are bounded. Includes a numerically stable
    implementation for the log-determinant of the Jacobian, essential for calculating log-probabilities.
    """
    domain = constraints.real
    codomain = constraints.interval(-1.0, 1.0)
    bijective = True
    sign = +1

    @staticmethod
    def atanh(x):
        return 0.5 * (x.log1p() - (-x).log1p())

    def __eq__(self, other):
        return isinstance(other, TanhTransform)

    def _call(self, x):
        return x.tanh()

    def _inverse(self, y):
        # Avoids clamping for better numerical stability with certain algorithms.
        return self.atanh(y)

    def log_abs_det_jacobian(self, x, y):
        # A more numerically stable formula for the log-abs-det-jacobian of tanh.
        return 2. * (math.log(2.) - x - F.softplus(-2. * x))


class MLPNetwork(nn.Module):
    """A standard Multi-Layer Perceptron (MLP) used as the backbone for various networks."""
    def __init__(self, input_dim, output_dim, hidden_size=256):
        super(MLPNetwork, self).__init__()
        self.network = nn.Sequential(
                        nn.Linear(input_dim, hidden_size),
                        nn.ReLU(),
                        nn.Linear(hidden_size, hidden_size),
                        nn.ReLU(),
                        nn.Linear(hidden_size, output_dim),
                        )

    def forward(self, x):
        return self.network(x)


class Actor(nn.Module):
    """
    The policy network (Actor) for SAC.
    It outputs the mean and log standard deviation of a Gaussian distribution,
    which is then transformed by a Tanh function to produce a stochastic action.
    """
    def __init__(self, state_dim, action_dim, hidden_size=256):
        super(Actor, self).__init__()
        # The network outputs two values for each action dimension: mean and log_std.
        self.network = MLPNetwork(state_dim, action_dim * 2, hidden_size)
        self.action_dim = action_dim

    def forward(self, state, get_log_prob=False):
        # Get mean and log_std from the network.
        mean, log_std = torch.chunk(self.network(state), 2, dim=-1)
        # Clamp log_std for numerical stability.
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)

        # Create a TransformedDistribution to handle the Tanh squashing.
        base_dist = Normal(mean, std)
        transforms = [TanhTransform(cache_size=1)]
        dist = TransformedDistribution(base_dist, transforms)

        # Sample an action using the reparameterization trick (rsample).
        action = dist.rsample()

        # Optionally compute the log probability of the sampled action.
        if get_log_prob:
            log_prob = dist.log_prob(action).sum(axis=-1, keepdim=True)
            return action, log_prob

        return action

    def evaluate(self, state, action):
        """
        Calculates the log-probability of a given action under the current policy.
        This is required for calculating the importance sampling ratio.
        """
        # Clamp action to be slightly inside the [-1, 1] range to avoid NaNs in atanh.
        clamped_action = action.clamp(-1 + 1e-6, 1 - 1e-6)
        # Invert the Tanh transformation to get the pre-squashed action.
        pre_tanh_action = TanhTransform()._inverse(clamped_action)

        # Get the distribution parameters from the network.
        mean, log_std = torch.chunk(self.network(state), 2, dim=-1)
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)
        base_dist = Normal(mean, std)

        # Calculate log_prob from the base Normal distribution for the pre-squashed action.
        log_prob = base_dist.log_prob(pre_tanh_action).sum(axis=-1, keepdim=True)

        # Correct the log_prob for the Tanh transformation using the log-determinant of the Jacobian.
        log_prob -= TanhTransform().log_abs_det_jacobian(pre_tanh_action, clamped_action).sum(axis=-1, keepdim=True)

        return log_prob


class Critic(nn.Module):
    """
    The Q-value network (Critic) for SAC.
    It takes a state and an action as input and outputs the estimated Q-value.
    """
    def __init__(self, state_dim, action_dim, hidden_size=256):
        super(Critic, self).__init__()
        self.network = MLPNetwork(state_dim + action_dim, 1, hidden_size)

    def forward(self, state, action):
        # Concatenate state and action to form the input to the Q-network.
        x = torch.cat((state, action), dim=1)
        return self.network(x)


# =================================================================================================
# --- NOMAD Agent Implementation ---
# =================================================================================================

class NOMAD(object):
    """
    The main NOMAD agent class. It orchestrates the dual-policy SAC algorithm, integrating
    DARC for source-domain training and LIBERTY for target-domain exploration.
    """
    def __init__(self,
                 config,
                 device,
                 target_entropy=None,
                 ):
        # --- Basic Agent Configuration ---
        self.device = device
        self.gamma = config['gamma']  # Discount factor for future rewards.
        self.tau = config['tau']      # Soft update coefficient for target networks.

        state_dim = config['state_dim']
        action_dim = config['action_dim']
        self.max_action = config['max_action']
        hidden_size = config['hidden_sizes']
        # Number of Q-functions for clipped double Q-learning. Defaults to 2.
        n_q_funcs = config.get('n_q_funcs', 2)

        # --- Main Policy Networks (for evaluation and source-domain training) ---
        self.policy = Actor(state_dim, action_dim, hidden_size).to(self.device)
        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=config['actor_lr'])
        self.q_funcs = nn.ModuleList([Critic(state_dim, action_dim, hidden_size) for _ in range(n_q_funcs)]).to(self.device)
        self.q_optimizer = torch.optim.Adam(self.q_funcs.parameters(), lr=config['critic_lr'])

        # --- Target Networks for Main Policy (for stable Q-learning targets) ---
        self.policy_target = copy.deepcopy(self.policy)
        self.q_funcs_target = copy.deepcopy(self.q_funcs)

        # --- Exploration Policy Networks (for target-domain interaction) ---
        self.exploration_policy = Actor(state_dim, action_dim, hidden_size).to(self.device)
        self.exploration_policy_optimizer = torch.optim.Adam(self.exploration_policy.parameters(), lr=config['actor_lr'])
        self.exploration_q_funcs = nn.ModuleList([Critic(state_dim, action_dim, hidden_size) for _ in range(n_q_funcs)]).to(self.device)
        self.exploration_q_optimizer = torch.optim.Adam(self.exploration_q_funcs.parameters(), lr=config['critic_lr'])
        self.exploration_q_funcs_target = copy.deepcopy(self.exploration_q_funcs)

        # --- SAC Temperature Configuration (for entropy maximization) ---
        self.use_auto_temp = config.get('use_auto_temp', True)
        if self.use_auto_temp:
            self.log_temp = torch.zeros(1, requires_grad=True, device=self.device)
            temp_lr = config.get('temp_lr', 3e-5)  # Learning rate for temperature.
            self.temp_optimizer = torch.optim.Adam([self.log_temp], lr=temp_lr)
            self.target_entropy = -float(action_dim) if target_entropy is None else target_entropy
            self.temp = self.log_temp.exp().item()
        else:
            self.temp = config['temp']  # Use a fixed temperature.

        # --- DARC Classifier (for source-domain reward correction) ---
        self.classifier = MLPNetwork(state_dim * 2 + action_dim, 1, hidden_size).to(self.device)
        self.classifier_optimizer = torch.optim.Adam(self.classifier.parameters(), lr=config.get('classifier_lr', 3e-4))
        self.darc_weight = config.get('darc_weight', 1.0)  # Weight for the DARC reward bonus.

        # --- LIBERTY Dynamics Models (for target-domain intrinsic reward) ---
        self.metric_model = MetricModel(state_dim, hidden_size).to(self.device)
        self.inverse_model = InverseDynamicsModel(state_dim, action_dim, hidden_size).to(self.device)
        self.forward_model = ForwardDynamicsModel(state_dim, action_dim, hidden_size).to(self.device)
        self.dynamics_optimizer = torch.optim.Adam(
            list(self.metric_model.parameters()) +
            list(self.inverse_model.parameters()) +
            list(self.forward_model.parameters()),
            lr=config.get('liberty_lr', 1e-3)
        )
        self.liberty_weight = config.get('liberty_weight', 1.0)     # Weight for the LIBERTY intrinsic reward.
        self.metric_loss_coeff = config.get('metric_loss_coeff', 0.2) # Coefficient for metric model loss.
        self.inv_loss_coeff = config.get('inv_loss_coeff', 0.8)       # Coefficient for inverse dynamics model loss.

        # --- Agent State Flags ---
        self.exploration_mode = False # Toggles between main and exploration policy for action selection.
        self.total_it = 0             # Total training iterations counter.

    def enable_exploration(self):
        """Switches the agent to use the exploration policy for action selection."""
        self.exploration_mode = True

    def disable_exploration(self):
        """Switches the agent to use the main policy for action selection."""
        self.exploration_mode = False

    def select_action(self, state, test=True):
        """Selects an action based on the current state and mode (test or train)."""
        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)

        if test:
            # During evaluation, use the main policy and take the deterministic mean of the action distribution.
            mean, _ = torch.chunk(self.policy.network(state), 2, dim=-1)
            action = torch.tanh(mean)
        else:
            # During training, select a policy based on the current exploration mode.
            if self.exploration_mode:
                policy_to_use = self.exploration_policy # Use for interacting with the target env.
            else:
                policy_to_use = self.policy             # Use for interacting with the source env.
            # Sample a stochastic action from the chosen policy.
            action = policy_to_use(state)

        return action.cpu().data.numpy().flatten() * self.max_action

    def train(self, src_replay_buffer, tar_replay_buffer, initial_state, batch_size=128, writer=None):
        """Performs a single training step for all components of the NOMAD agent."""
        self.total_it += 1

        # Sample minibatches from both source and target replay buffers.
        src_state, src_action, src_next_state, src_reward, src_not_done = src_replay_buffer.sample(batch_size)
        tar_state, tar_action, tar_next_state, tar_reward, tar_not_done = tar_replay_buffer.sample(batch_size)

        # =========================================================================================
        # 1. Train LIBERTY Dynamics Models on Target Data
        #    These models learn the dynamics of the target domain to generate intrinsic rewards.
        # =========================================================================================
        # Inverse dynamics: predict action given state and next_state.
        pred_action = self.inverse_model(tar_state, tar_next_state)
        inv_loss = F.mse_loss(pred_action, tar_action)

        # Forward dynamics: predict next_state distribution given state and action.
        # The model returns (mu, log_std), so we must construct the distribution object.
        mu, log_std = self.forward_model(tar_state, tar_action)
        # Clamp log_std for numerical stability.
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)
        pred_next_state_dist = Normal(mu, std) # Create the distribution here
        fwd_loss = -pred_next_state_dist.log_prob(tar_next_state.detach()).mean()

        # Bisimulation metric: learn a distance metric between states.
        s_i, s_j = torch.split(tar_state, batch_size // 2, dim=0)
        s_prime_i, s_prime_j = torch.split(tar_next_state, batch_size // 2, dim=0)
        a_i, a_j = torch.split(tar_action, batch_size // 2, dim=0)
        dist_s = self.metric_model(s_i, s_j)
        dist_s_prime = self.metric_model(s_prime_i, s_prime_j)
        action_diff = torch.mean(torch.pow(a_i - a_j, 2), dim=1, keepdim=True)
        metric_loss = (F.mse_loss(dist_s, (action_diff + self.gamma * dist_s_prime).detach()))

        # Combine losses and update the dynamics models.
        dynamics_loss = fwd_loss + self.inv_loss_coeff * inv_loss + self.metric_loss_coeff * metric_loss
        self.dynamics_optimizer.zero_grad()
        dynamics_loss.backward()
        self.dynamics_optimizer.step()

        # =========================================================================================
        # 2. Train Exploration Policy on Target Data with LIBERTY Intrinsic Rewards
        #    This policy learns to explore the target domain effectively.
        # =========================================================================================
        with torch.no_grad():
            # Calculate potential-based intrinsic reward using the metric model.
            s0 = torch.FloatTensor(initial_state).to(self.device).unsqueeze(0).repeat(tar_state.shape[0], 1)
            potential_s = self.metric_model(tar_state, s0)
            potential_s_next = self.metric_model(tar_next_state, s0)
            intrinsic_reward = self.gamma * potential_s_next - potential_s
            tar_reward_intrinsic = tar_reward + self.liberty_weight * intrinsic_reward

            # Calculate the target Q-value for the exploration critic.
            next_actions_tar, next_log_probs_tar = self.exploration_policy(tar_next_state, get_log_prob=True)
            q_target_next_list = [q_target(tar_next_state, next_actions_tar) for q_target in self.exploration_q_funcs_target]
            q_target_next = torch.min(torch.hstack(q_target_next_list), axis=1, keepdim=True).values
            q_target = tar_reward_intrinsic + tar_not_done * self.gamma * (q_target_next - self.temp * next_log_probs_tar)

        # Update the exploration critic.
        q_list = [q(tar_state, tar_action) for q in self.exploration_q_funcs]
        q_val = torch.hstack(q_list)
        exploration_q_loss = sum(F.mse_loss(q_val[:, i:i+1], q_target) for i in range(len(self.exploration_q_funcs)))
        self.exploration_q_optimizer.zero_grad()
        exploration_q_loss.backward()
        self.exploration_q_optimizer.step()

        # Update the exploration actor (policy).
        actions_pred_tar, log_probs_tar = self.exploration_policy(tar_state, get_log_prob=True)
        q_pi_list = [q(tar_state, actions_pred_tar) for q in self.exploration_q_funcs]
        q_pi = torch.min(torch.hstack(q_pi_list), axis=1, keepdim=True).values
        exploration_policy_loss = (self.temp * log_probs_tar - q_pi).mean()
        self.exploration_policy_optimizer.zero_grad()
        exploration_policy_loss.backward()
        self.exploration_policy_optimizer.step()

        # =========================================================================================
        # 3. Train DARC Classifier to Distinguish Source vs. Target Transitions
        # =========================================================================================
        src_trans = torch.cat([src_state, src_action, src_next_state], dim=1)
        tar_trans = torch.cat([tar_state, tar_action, tar_next_state], dim=1)
        src_logits = self.classifier(src_trans)
        tar_logits = self.classifier(tar_trans)
        labels = torch.cat([torch.zeros_like(src_logits), torch.ones_like(tar_logits)]) # Source=0, Target=1
        logits = torch.cat([src_logits, tar_logits])
        classifier_loss = F.binary_cross_entropy_with_logits(logits, labels)
        self.classifier_optimizer.zero_grad()
        classifier_loss.backward()
        self.classifier_optimizer.step()

        # =========================================================================================
        # 4. Train Main Policy (SAC with DARC on Source, IS on Target)
        # =========================================================================================
        with torch.no_grad():
            # --- Importance Sampling Ratio for Target Data ---
            # Corrects for the fact that target data was collected by the exploration policy,
            # not the main policy we are currently training.
            log_prob_main_pi = self.policy.evaluate(tar_state, tar_action)
            log_prob_explore_pi = self.exploration_policy.evaluate(tar_state, tar_action)
            is_ratio = torch.exp(log_prob_main_pi - log_prob_explore_pi).clamp(0.0, 5.0) # Clamp for stability

            # --- DARC Reward Correction for Source Data ---
            # Penalizes transitions that the classifier identifies as source-like.
            darc_reward = -F.logsigmoid(-self.classifier(src_trans))
            src_reward_corrected = src_reward + self.darc_weight * darc_reward

            # --- Target Q-value Calculation ---
            # For the source batch (using corrected rewards).
            next_actions_src, next_log_probs_src = self.policy(src_next_state, get_log_prob=True)
            q_target_next_src_list = [q_target(src_next_state, next_actions_src) for q_target in self.q_funcs_target]
            q_target_next_src = torch.min(torch.hstack(q_target_next_src_list), axis=1, keepdim=True).values
            q_target_src = src_reward_corrected + src_not_done * self.gamma * (q_target_next_src - self.temp * next_log_probs_src)

            # For the target batch (using original rewards).
            next_actions_tar_main, next_log_probs_tar_main = self.policy(tar_next_state, get_log_prob=True)
            q_target_next_tar_list = [q_target(tar_next_state, next_actions_tar_main) for q_target in self.q_funcs_target]
            q_target_next_tar = torch.min(torch.hstack(q_target_next_tar_list), axis=1, keepdim=True).values
            q_target_tar = tar_reward + tar_not_done * self.gamma * (q_target_next_tar - self.temp * next_log_probs_tar_main)

            q_target_combined = torch.cat([q_target_src, q_target_tar], dim=0)

        # --- Update Main Critic (Q-Functions) ---
        combined_state = torch.cat([src_state, tar_state], dim=0)
        combined_action = torch.cat([src_action, tar_action], dim=0)
        q_list_combined = [q(combined_state, combined_action) for q in self.q_funcs]
        q_val_combined = torch.hstack(q_list_combined)
        q_val_src, q_val_tar = q_val_combined[:batch_size], q_val_combined[batch_size:]
        q_target_src, q_target_tar = q_target_combined[:batch_size], q_target_combined[batch_size:]

        # Standard MSE loss for the source batch.
        src_q_loss = sum(F.mse_loss(q_val_src[:, i:i+1], q_target_src) for i in range(len(self.q_funcs)))

        # Importance-weighted MSE loss for the target batch.
        tar_q_losses_unweighted = [F.mse_loss(q_val_tar[:, i:i+1], q_target_tar, reduction='none') for i in range(len(self.q_funcs))]
        tar_q_loss = sum((is_ratio * loss).mean() for loss in tar_q_losses_unweighted)

        q_loss = src_q_loss + tar_q_loss
        self.q_optimizer.zero_grad()
        q_loss.backward()
        self.q_optimizer.step()

        # --- Update Main Actor (Policy) ---
        # Actor is updated on both source and target data to maximize the Q-function.
        actions_pred, log_probs = self.policy(combined_state, get_log_prob=True)
        q_pi_list = [q(combined_state, actions_pred) for q in self.q_funcs]
        q_pi = torch.min(torch.hstack(q_pi_list), axis=1, keepdim=True).values
        policy_loss = (self.temp * log_probs - q_pi).mean()
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        # --- Update Temperature (if using auto-tuning) ---
        if self.use_auto_temp:
            temp_loss = -self.log_temp * (log_probs.detach() + self.target_entropy).mean()
            self.temp_optimizer.zero_grad()
            temp_loss.backward()
            self.temp_optimizer.step()
            self.temp = self.log_temp.exp().item()

        # =========================================================================================
        # 5. Soft Update Target Networks
        #    Slowly track the weights of the main networks to stabilize training.
        # =========================================================================================
        for target_param, param in zip(self.q_funcs_target.parameters(), self.q_funcs.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)
        for target_param, param in zip(self.policy_target.parameters(), self.policy.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)
        for target_param, param in zip(self.exploration_q_funcs_target.parameters(), self.exploration_q_funcs.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

        # =========================================================================================
        # 6. Log Metrics to TensorBoard for Monitoring
        # =========================================================================================
        if writer is not None and self.total_it % 100 == 0:
            writer.add_scalar('loss/main_q_loss', q_loss.item(), self.total_it)
            writer.add_scalar('loss/main_policy_loss', policy_loss.item(), self.total_it)
            writer.add_scalar('loss/exploration_q_loss', exploration_q_loss.item(), self.total_it)
            writer.add_scalar('loss/exploration_policy_loss', exploration_policy_loss.item(), self.total_it)
            writer.add_scalar('loss/classifier_loss', classifier_loss.item(), self.total_it)
            writer.add_scalar('loss/dynamics_loss', dynamics_loss.item(), self.total_it)
            writer.add_scalar('reward/darc_reward_mean', darc_reward.mean().item(), self.total_it)
            writer.add_scalar('reward/liberty_intrinsic_reward_mean', intrinsic_reward.mean().item(), self.total_it)
            writer.add_scalar('param/temperature', self.temp, self.total_it)
            writer.add_scalar('param/is_ratio_mean', is_ratio.mean().item(), self.total_it)

    def save(self, filename):
        """Saves all model and optimizer states to disk."""
        torch.save(self.q_funcs.state_dict(), filename + "_critic")
        torch.save(self.q_optimizer.state_dict(), filename + "_critic_optimizer")
        torch.save(self.policy.state_dict(), filename + "_actor")
        torch.save(self.policy_optimizer.state_dict(), filename + "_actor_optimizer")
        torch.save(self.exploration_policy.state_dict(), filename + "_exploration_actor")
        torch.save(self.exploration_policy_optimizer.state_dict(), filename + "_exploration_actor_optimizer")
        torch.save(self.classifier.state_dict(), filename + "_classifier")
        torch.save(self.classifier_optimizer.state_dict(), filename + "_classifier_optimizer")
        torch.save(self.metric_model.state_dict(), filename + "_metric_model")
        torch.save(self.inverse_model.state_dict(), filename + "_inverse_model")
        torch.save(self.forward_model.state_dict(), filename + "_forward_model")
        torch.save(self.dynamics_optimizer.state_dict(), filename + "_dynamics_optimizer")

    def load(self, filename):
        """Loads all model and optimizer states from disk."""
        self.q_funcs.load_state_dict(torch.load(filename + "_critic"))
        self.q_optimizer.load_state_dict(torch.load(filename + "_critic_optimizer"))
        self.q_funcs_target = copy.deepcopy(self.q_funcs)
        self.policy.load_state_dict(torch.load(filename + "_actor"))
        self.policy_optimizer.load_state_dict(torch.load(filename + "_actor_optimizer"))
        self.policy_target = copy.deepcopy(self.policy)
        self.exploration_policy.load_state_dict(torch.load(filename + "_exploration_actor"))
        self.exploration_policy_optimizer.load_state_dict(torch.load(filename + "_exploration_actor_optimizer"))
        self.classifier.load_state_dict(torch.load(filename + "_classifier"))
        self.classifier_optimizer.load_state_dict(torch.load(filename + "_classifier_optimizer"))
        self.metric_model.load_state_dict(torch.load(filename + "_metric_model"))
        self.inverse_model.load_state_dict(torch.load(filename + "_inverse_model"))
        self.forward_model.load_state_dict(torch.load(filename + "_forward_model"))
        self.dynamics_optimizer.load_state_dict(torch.load(filename + "_dynamics_optimizer"))


